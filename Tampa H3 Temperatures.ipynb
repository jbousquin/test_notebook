{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes an area of interest (Tampa Bay, FL) and a csv of water quality data (temperature). It starts by walking through how to fill that area of interest polygon with H3 Heaxagons of different sizes. It then walks through aggregating water quality data to a given scale of hexagons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import os\n",
    "import pandas\n",
    "import numpy\n",
    "import geopandas\n",
    "import h3\n",
    "import matplotlib.pyplot as plt\n",
    "# from shapely.geometry import mapping\n",
    "from shapely.geometry import shape, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_hex_idx(gdf, aperture_size):\n",
    "    \"\"\" Polygon to hex by fill (only 1 polygon at a time)\"\"\"\n",
    "    hex_col = 'hex' + str(aperture_size)\n",
    "    temp = mapping(gdf)  #shapely.mapping()\n",
    "    temp_geo = temp['features'][0]['geometry']\n",
    "    # Switch temp_geo x-y order and convert tuple to list\n",
    "    temp_geo['coordinates'] = [[(j[1],j[0]) for j in geo] for geo in temp_geo['coordinates']]\n",
    "    # Create dataframe for polygon filled with hexagons\n",
    "    df_hex = pandas.DataFrame(h3.polyfill(temp_geo,\n",
    "                                          aperture_size),\n",
    "                              columns=[hex_col])\n",
    "\n",
    "    return df_hex\n",
    "\n",
    "\n",
    "def boundary_geom(hex_id):\n",
    "    '''Implement h3_to_geo_boundary in bulk'''\n",
    "    return shape({'type': 'Polygon',\n",
    "                  'coordinates': [h3.h3_to_geo_boundary(h=hex_id,\n",
    "                                                        geo_json=True)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Area of Interest (AOI) shapefile from temperature_data folder in current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'temperature_data')\n",
    "shp_aoi = os.path.join(data_dir, 'TampaBay.shp')\n",
    "gdf_aoi = geopandas.read_file(shp_aoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a geodata with the hexagon bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired aperture size (1-15)\n",
    "aperture_size = 11\n",
    "hex_col = 'hex' + str(aperture_size)\n",
    "# Create data frame of hex ids of the set size that fill the aoi\n",
    "aoi_hex_fill = poly_hex_idx(gdf_aoi, aperture_size)\n",
    "# To map these hexagons they need planar coordinates\n",
    "aoi_hex_fill['geom'] = aoi_hex_fill[hex_col].apply(lambda x: boundary_geom(x))\n",
    "# The boundary coordinates can be used to create a geopandas dataframe\n",
    "gdf_hex = geopandas.GeoDataFrame(aoi_hex_fill, geometry='geom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geopandas can easily be saved as a shapefile\n",
    "out_shp = os.path.join(data_dir, 'fill_{}.shp'.format(hex_col))\n",
    "gdf_hex.to_file(out_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the example point data (downloads as two parts: points shp and csv data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_combine(df, date_field, time_field, tz_field, datetime_field):\n",
    "    \"\"\"from USGS data retrieval (for NWIS)\"\"\"\n",
    "    # Time zone converter dictionary\n",
    "    tz_dict = {'EST': '-0500',\n",
    "               'EDT': '-0400',\n",
    "               numpy.nan: ''}\n",
    "    df[tz_field] = df[tz_field].map(tz_dict)\n",
    "    df[time_field] = df[time_field].fillna('00:00:00')\n",
    "    \n",
    "    \n",
    "    # datetime formated\n",
    "    df[datetime_field] = pandas.to_datetime(df.pop(date_field) + ' ' +\n",
    "                                            df.pop(time_field) + ' ' +\n",
    "                                            df.pop(tz_field),\n",
    "                                            format='%Y-%m-%d %H:%M',\n",
    "                                            utc=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def temp_F2C(temp_F):\n",
    "    \"\"\"Convert temperture from Fahrenheit to Celsius\"\"\"\n",
    "    return (temp_F - 32.0) * 5.0 / 9.0\n",
    "\n",
    "\n",
    "def convert_temperature(df, val_field, unit_field, new_field=False):\n",
    "    \"\"\"Convert temperature field to consistent degrees C\n",
    "    if new_field specified dataframe updated inplace and old fields dropped\n",
    "    if no new_field specified (default) returns series with converted values\n",
    "    \"\"\"\n",
    "    unit_mask = df[unit_field] == 'deg F'\n",
    "    if new_field:\n",
    "        df[new_field] = df[val_field].mask(unit_mask, temp_F2C(df[val_field]))\n",
    "        df.drop([val_field, unit_field], axis=1, inplace=True)\n",
    "    else:\n",
    "        return df[val_field].mask(unit_mask, temp_F2C(df[val_field]))\n",
    "\n",
    "\n",
    "def wqp_stats(df, datetime_field, statReportType='daily', statTypeCd='mean'):\n",
    "    \"\"\"\n",
    "    NWIS-like daily statistics funciton for waterquality portal\n",
    "    \n",
    "    Parameters:     \n",
    "        statReportType (string): daily (default), monthly, or annual\n",
    "        statTypeCd (string): all, mean, max, min, median\n",
    "    \n",
    "        Start and End will always be min/max timestamp in dataframe\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe of summary statistics\n",
    "    \"\"\"\n",
    "    pd_freq = {'daily': 'D',\n",
    "               'monthly': 'M',\n",
    "               'annual': 'A'\n",
    "               }\n",
    "    if statReportType in pd_freq.keys():\n",
    "        freq = pd_freq[statReportType]\n",
    "    else:\n",
    "        freq = statReportType\n",
    "    \n",
    "    # Loc_ID field to groupby (deal with depth later)\n",
    "    df['Loc_ID'] = df.index\n",
    "\n",
    "    # Resample by set increment\n",
    "    if statTypeCd == 'all':\n",
    "        #return all stats as multiple columns, loop over self call?\n",
    "        return df\n",
    "    elif statTypeCd == 'mean':\n",
    "        return df.groupby('Loc_ID').resample(freq, on=datetime_field).mean()\n",
    "    elif statTypeCd == 'max':\n",
    "        return df.groupby('Loc_ID').resample(freq, on=datetime_field).max()\n",
    "    elif statTypeCd == 'min':\n",
    "        return df.groupby('Loc_ID').resample(freq, on=datetime_field).min()\n",
    "    elif statTypeCd == 'median':\n",
    "        return df.groupby('Loc_ID').resample(freq, on=datetime_field).median()\n",
    "\n",
    "\n",
    "def hex_idx(gdf, aperture_size, method='count'):\n",
    "    \"\"\" Point to hex by aggregating\"\"\"\n",
    "    hex_col = 'hex' + str(aperture_size)\n",
    "    # Assign hex ID\n",
    "    gdf[hex_col] = gdf.apply(lambda x: h3.geo_to_h3(x.geometry.y,\n",
    "                                                    x.geometry.x,\n",
    "                                                    aperture_size), 1)\n",
    "    # Aggregate into hexagons\n",
    "    if method=='avg':\n",
    "        df_hex = gdf.groupby(hex_col).mean()\n",
    "    else:\n",
    "        df_hex = gdf.groupby(hex_col).size().to_frame('cnt')\n",
    "    return df_hex.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WQP station points shapefile\n",
    "shp_loc = os.path.join(data_dir, 'station_points_combined.shp')\n",
    "gdf_loc = geopandas.read_file(shp_loc)  # Read to geodataframe\n",
    "# Temperature results csv\n",
    "csv_temp = os.path.join(data_dir, 'narrowresult.csv')\n",
    "# Change Coordinate Reference System to match AOI if it doesn't already\n",
    "if gdf_loc.crs != gdf_aoi.crs:\n",
    "    gdf_aoi.to_crs(gdf_loc.crs.to_epsg(), inplace=True)\n",
    "# Sample points are retrieved by extent, so clip it to the AOI polygon\n",
    "gdf = geopandas.clip(gdf_loc, gdf_aoi)\n",
    "gdf.drop(['Id'], axis=1, inplace=True)  # Drop 'Id' it's an empty field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv to a df, dropping empty fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are already in sample points table and not used for join\n",
    "dup_cols = ['OrganizationIdentifier', 'OrganizationFormalName', 'ProviderName']\n",
    "# Read csv to dataframe\n",
    "df_temp = pandas.read_csv(csv_temp,\n",
    "                          usecols=lambda x: x not in dup_cols,\n",
    "                          low_memory=False)\n",
    "# Drop missing data columns\n",
    "df_temp.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse date fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = datetime_combine(df_temp,\n",
    "                           'ActivityStartDate',\n",
    "                           'ActivityStartTime/Time',\n",
    "                           'ActivityStartTime/TimeZoneCode',\n",
    "                           'Activity_datetime')\n",
    "# drop '0001-01-01' from time stamps\n",
    "analysis = 'AnalysisStartTime/Time'\n",
    "df_temp[analysis] = [t[11:] if str(t).startswith('0001-01-01') else t\n",
    "                     for t in df_temp[analysis]]\n",
    "df_temp = datetime_combine(df_temp,\n",
    "                           'AnalysisStartDate',\n",
    "                           'AnalysisStartTime/Time',\n",
    "                           'AnalysisStartTime/TimeZoneCode',\n",
    "                           'Analysis_datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join sample points and temperature results on 'Loc_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_temp = gdf.set_index('Loc_ID').join(df_temp.set_index('MonitoringLocationIdentifier'))\n",
    "#NOTE: if renamed so both are Loc_ID the index will have a name\n",
    "gdf_temp.dropna(axis=1, how='all', inplace=True)  # Drop empty column again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert temperature results to consistent units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_temperature(gdf_temp,\n",
    "                    'ResultMeasureValue',\n",
    "                    'ResultMeasure/MeasureUnitCode',\n",
    "                    'deg_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime fields can be used to get specific periods of time and average across set increments (e.g. average daily value from hourly data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get annual averages\n",
    "gdf_avg = wqp_stats(gdf_temp, 'Activity_datetime', statReportType='annual')\n",
    "# Limit to 2011\n",
    "gdf_avg.reset_index(inplace=True)\n",
    "df_2011 = gdf_avg[gdf_avg['Activity_datetime']=='2011-12-31 00:00:00+00:00']\n",
    "# Join back to geometry\n",
    "gdf_2011 = gdf.set_index('Loc_ID').join(df_2011.set_index('Loc_ID'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert point temperatures to hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired aperture size (1-15)\n",
    "aperture_size = 11\n",
    "hex_col = 'hex' + str(aperture_size)\n",
    "# Values within one hex can be aggregated multiple ways\n",
    "df_hex = hex_idx(gdf, aperture_size)  # Count - to get a count of data points\n",
    "hex_2011 = hex_idx(gdf_2011, aperture_size,'avg')  # Average of temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add geometry back to the dataframe to map it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hex['geom'] = df_hex[hex_col].apply(lambda x: boundary_geom(x))\n",
    "gdf_hex = geopandas.GeoDataFrame(df_hex, geometry='geom')\n",
    "\n",
    "hex_2011['geom'] = hex_2011[hex_col].apply(lambda x: boundary_geom(x))\n",
    "gdf_2011_hex = geopandas.GeoDataFrame(hex_2011, geometry='geom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate areas where temperatures are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kring_IDW(df, hex_col, metric_col, k_max):\n",
    "    '''\n",
    "    Uses hollow rings method, solid disks may be faster\n",
    "    k_max is the number of rings out from a cell to consider\n",
    "    '''\n",
    "    df_agg = df[[hex_col]]\n",
    "    df_agg['hexk'] = df_agg[hex_col]\n",
    "    df_agg.set_index(hex_col,inplace=True)\n",
    "    temp2 = [df_agg['hexk'].reset_index()]\n",
    "    temp2[-1]['k'] = 0\n",
    "\n",
    "    for k in range(1,k_max):\n",
    "        temp2.append((df_agg['hexk']\n",
    "                     .apply(lambda x: pandas.Series(list(h3.hex_ring(x,k)))).stack()\n",
    "                     .to_frame('hexk').reset_index(1, drop=True).reset_index()\n",
    "                ))\n",
    "        temp2[-1]['k'] = k\n",
    "    df_all = pandas.concat(temp2).merge(df)\n",
    "    df_all.dropna(subset=[metric_col],inplace=True)  # Drop NaN\n",
    "    # Instead of applying the coef, divide by k\n",
    "    #df_all[metric_col] = df_all[metric_col] * df_all.k.apply(lambda x:coef[x])\n",
    "    df_all['num_val'] = df_all[metric_col]/df_all.k  # if k=0, x/k = inf\n",
    "    # sum numerator\n",
    "    dfs_t = df_all.groupby('hexk')[['num_val']].sum()\n",
    "    # sum denominator\n",
    "    df_all['den_val'] = 1.0/df_all.k\n",
    "    #nan -> inf so convert back and drop nan\n",
    "    df_all['den_val'] = df_all['den_val'].replace(numpy.inf, numpy.nan)\n",
    "    #df_all.dropna(subset=['num_val'],inplace=True)\n",
    "    dfs_b = df_all.groupby('hexk')[['den_val']].sum()\n",
    "\n",
    "    dfs = dfs_t.join(dfs_b)\n",
    "    dfs['idw'] = dfs['num_val']/dfs['den_val']\n",
    "    #reduce dfs\n",
    "    dfs = dfs[['idw']]\n",
    "    #fix index name\n",
    "    dfs.index.rename(hex_col,inplace=True)\n",
    "    #join back to df\n",
    "    dfs = df.set_index(hex_col).join(dfs)\n",
    "    #replace inf with orig?\n",
    "    return dfs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filled area (define where temperatures are wanted)\n",
    "aoi_hex_fill = poly_hex_idx(gdf_aoi, aperture_size)\n",
    "# Combine hex data and filled hex\n",
    "df_all = aoi_hex_fill.set_index(hex_col).join(gdf_2011_hex.set_index(hex_col))\n",
    "df_all.reset_index(inplace=True)\n",
    "# Fill missing cells using IDW\n",
    "df_2011_idw = kring_IDW(df_all, hex_col, metric_col='deg_c', k_max=20)\n",
    "# Add geometry to hexagons to map\n",
    "df_2011_idw['geom'] = df_2011_idw[hex_col].apply(lambda x: boundary_geom(x))\n",
    "gdf_2011_idw = geopandas.GeoDataFrame(df_2011_idw, geometry='geom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check kring_IDW for something being done on a slice instead of the df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
